### 爬虫学习路径

#### 1、学习Python包并实现基本的爬虫过程

大部分爬虫都是按 “发送请求——获得页面——解析页面——抽取并储存内容” 流程来进行，这其实也是模拟了我们使用浏览器获取网页信息的过程。

Python爬虫的相关包很多：urllib、requests、bs4、scrapy、pyspider等，建议从 requests+xpath 开始，requests 负责连接网站，返回网页，xpath 用于解析网页，便于抽取数据。

如果需要爬取异步加载的网站，可以学习浏览器抓包分析真实请求或者学习 selenium 来实现自动化，这样知乎、时光网、猫途鹰这些动态网站也可以迎刃而解。

#### 2、了解非结构化数据的存储

爬回来的数据可以直接用文档形式存在本地，也可以存入数据库中。

开始数据量不大的时候，可以直接通过 Python 的语法或 pandas 的方法将数据村委csv文件。

当然，可能爬回来的数据并不是干净的，可能会有缺失、错误等等，还需要对数据进行清洗，可以学习 pandas 包的基本用法来做数据的预处理，得到更干净的数据。

#### 3、学习scrapy，搭建工程化的爬虫

掌握前面的技术一般量级的数据和代码基本没有问题了，但是遇到非常复杂的情况，可能仍然会力不从心，这个时候，强大的 scrapy 框架就非常有用了。

scrapy 是一个功能非常强大的爬虫框架，它不仅能便捷地构建 request，还有强大的 selector 能够方便地解析 response，然而它最让人惊喜的还是它超高的性能，可以将爬虫工程化、模块化。

学会 scrapy，可以自己取搭建一些爬虫框架，就基本具备爬虫工程师的思维了。

#### 4、学习数据库基础，应对大规模数据存储

爬回来的数据量小的时候，可以用文档的形式来存储。一旦数据量大了，这就有点行不通了。所以掌握一种数据库是必须的，学习目前比较主流的 MongoDB 就ok。

MongoDB 可以方便你去存储一些非结构化的数据，比如各种评论的文本、图片链接等。也可以利用 PyMongo，更方便地在 Python 中操作 MongoDB。

这里用到的数据库知识简单，主要是数据如何入库、如何提取。

#### 5、掌握各种技巧，应对特殊网站的反爬措施

爬虫过程也会遇到网站封IP、各种奇怪的验证码、useragent访问限制、各种动态加载等等。

遇到这些反爬虫的手段，还需要一些高级的技巧来应对，常规的比如访问频率控制、使用代理IP池、抓包、验证码的OCR处理等等。

往往网站在高效开发和反爬虫之间会偏向前者，这为爬虫提供了空间，掌握这些应对反爬虫的技巧，绝大部分的网站已难不到你。

#### 6、分布式爬虫，实现大规模并发采集

爬取基本数据已不是问题，瓶颈会集中到爬取海量数据的效率。这个时候，得学会分布式爬虫。

分布式，其实就是利用多线程的原来让多个爬虫同时工作，需要掌握 scrapy+MongoDB+Redis 这三种工具。scrapy 基本页面的爬取，MongoDB 存储爬取的数据，Redis 存储要爬取的网页队列，也就是任务队列。